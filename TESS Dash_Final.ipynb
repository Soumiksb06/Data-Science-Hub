{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbef33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumi\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:1540/\n",
      "\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:1540\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [22/May/2023 16:27:50] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/May/2023 16:27:50] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/May/2023 16:27:50] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/May/2023 16:27:50] \"GET /_dash-component-suites/dash/dcc/async-upload.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [22/May/2023 16:27:50] \"GET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [22/May/2023 16:27:50] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/May/2023 16:27:50] \"GET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/May/2023 16:28:04] \"POST /_dash-update-component HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "import librosa\n",
    "import numpy as np\n",
    "import base64\n",
    "import pickle\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create the Dash app\n",
    "app = dash.Dash(__name__, title='Toronto Speech Emotion Recognizer')\n",
    "\n",
    "# Create the uploaded_audio directory if it doesn't exist\n",
    "if not os.path.exists(\"uploaded_audio\"):\n",
    "    os.makedirs(\"uploaded_audio\")\n",
    "\n",
    "app.layout = html.Div(\n",
    "    style={'backgroundColor': 'LightBlue', 'padding': '30px'},\n",
    "    children=[\n",
    "        html.H1(\n",
    "            \"Emotion Prediction\",\n",
    "            style={'textAlign': 'center', 'color': '333', 'fontFamily': 'Arial, sans-serif'}\n",
    "        ),\n",
    "        dcc.Upload(\n",
    "            id='upload-audio',\n",
    "            children=html.Div([\n",
    "                'Drag and Drop or ',\n",
    "                html.A('Select Audio File')\n",
    "            ]),\n",
    "            style={\n",
    "                'width': '50%',\n",
    "                'height': '60px',\n",
    "                'lineHeight': '60px',\n",
    "                'borderWidth': '2px',\n",
    "                'borderStyle': 'dashed',\n",
    "                'borderRadius': '5px',\n",
    "                'textAlign': 'center',\n",
    "                'margin': '10px',\n",
    "                'backgroundColor': 'Gold',\n",
    "                'color': '333',\n",
    "                'fontFamily': 'Arial, sans-serif'\n",
    "            },\n",
    "            multiple=False\n",
    "        ),\n",
    "        html.Button(\n",
    "            'Record Audio',\n",
    "            id='record-audio-button',\n",
    "            style={'margin': '10px'}\n",
    "        ),\n",
    "        html.Div(\n",
    "            id='output-prediction',\n",
    "            style={'marginTop': '20px', 'fontFamily': 'Arial sans-serif', 'fontSize': '20px'}\n",
    "        ),\n",
    "        dcc.Graph(\n",
    "            id='spectrogram-graph',\n",
    "            style={'marginTop': '20px'}\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def extract_mfcc(audio, sr):\n",
    "    duration = 3  # adjust duration if necessary\n",
    "    offset = 0.5  # adjust offset if necessary\n",
    "\n",
    "    audio, _ = librosa.load(audio, sr=sr, duration=duration, offset=offset)\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40).T, axis=0)\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)\n",
    "    mfcc = np.expand_dims(mfcc, axis=-1)\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('output-prediction', 'children'),\n",
    "    dash.dependencies.Output('spectrogram-graph', 'figure'),\n",
    "    [dash.dependencies.Input('upload-audio', 'contents')],\n",
    "    [dash.dependencies.State('upload-audio', 'filename')],\n",
    "    [dash.dependencies.Input('record-audio-button', 'n_clicks')]\n",
    ")\n",
    "def predict_emotion(contents, filename, n_clicks):\n",
    "    if contents is not None:\n",
    "        content_type, content_string = contents.split(',')\n",
    "\n",
    "        audio_path = f\"./uploaded_audio/{filename}\"\n",
    "        with open(audio_path, 'wb') as f:\n",
    "            f.write(base64.b64decode(content_string))\n",
    "\n",
    "        audio, sr = librosa.load(audio_path, sr=None)  # Load audio with original sample rate\n",
    "    elif n_clicks is not None and n_clicks > 0:\n",
    "        # Recording parameters\n",
    "        duration = 3  # adjust duration if necessary\n",
    "        default_sr = sd.query_devices(None, 'input')['default_samplerate']\n",
    "        channels = 1\n",
    "\n",
    "        print(\"Recording started...\")\n",
    "        audio = sd.rec(int(duration * default_sr), samplerate=default_sr, channels=channels)\n",
    "        sd.wait()  # Wait until recording is finished\n",
    "        print(\"Recording finished.\")\n",
    "\n",
    "        # Save the recorded audio to a file\n",
    "        audio_path = \"./uploaded_audio/recorded_audio.wav\"\n",
    "        write(audio_path, default_sr, audio)\n",
    "\n",
    "        sr = default_sr  # Use the default sample rate for MFCC extraction\n",
    "    else:\n",
    "        return '', {}\n",
    "\n",
    "    # Perform emotion prediction\n",
    "    mfcc_features = extract_mfcc(audio_path, sr)\n",
    "    prediction = model.predict(mfcc_features)\n",
    "    predicted_label = enc.inverse_transform(prediction)\n",
    "    predicted_emotion = predicted_label[0][0].upper()\n",
    "\n",
    "    # Generate spectrogram\n",
    "    _, _, spectrogram = librosa.reassigned_spectrogram(audio, sr=sr)\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=spectrogram,\n",
    "        colorscale='Hot',\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Spectrogram',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Frequency',\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        html.H3(\n",
    "            f\"The predicted emotion is: {predicted_emotion}\",\n",
    "            style={'color': 'red', 'textAlign': 'center'}\n",
    "        ),\n",
    "        fig\n",
    "    )\n",
    "\n",
    "\n",
    "    # Perform emotion prediction\n",
    "    mfcc_features = extract_mfcc(audio_path, sr)\n",
    "    prediction = model.predict(mfcc_features)\n",
    "    predicted_label = enc.inverse_transform(prediction)\n",
    "    predicted_emotion = predicted_label[0][0].upper()\n",
    "\n",
    "    # Generate spectrogram\n",
    "    _, _, spectrogram = librosa.reassigned_spectrogram(audio, sr=sr)\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=spectrogram,\n",
    "        colorscale='Hot',\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Spectrogram',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Frequency',\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        html.H3(\n",
    "            f\"The predicted emotion is: {predicted_emotion}\",\n",
    "            style={'color': 'red', 'textAlign': 'center'}\n",
    "        ),\n",
    "        fig\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model and encoder\n",
    "    model = load_model(\"C:/Users/soumi/Downloads/TESS_latest_trained_model.h5\")  # Replace with your trained model file path\n",
    "\n",
    "    with open(\"C:/Users/soumi/Downloads/TESS_encoder.pkl\", 'rb') as f:\n",
    "        enc = pickle.load(f)\n",
    "\n",
    "    app.run_server(debug=False, port=1540)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e5cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
